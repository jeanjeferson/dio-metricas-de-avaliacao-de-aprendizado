{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay-tHHe9zdJd"
   },
   "source": [
    "Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1733252155649,
     "user": {
      "displayName": "Jean Moura",
      "userId": "07584647446128205886"
     },
     "user_tz": 180
    },
    "id": "zFzGyJUFzqYr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montar drive no Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "# Parâmetros de divisão do dataset\n",
    "TRAIN_SPLIT = 0.7     # 70% para treino\n",
    "VAL_SPLIT = 0.15      # 15% para validação\n",
    "TEST_SPLIT = 0.15     # 15% para teste\n",
    "\n",
    "# Verificação de segurança para garantir que os splits somam 1\n",
    "assert abs(TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT - 1.0) < 1e-9, \"Os splits devem somar 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_dir, train_split=TRAIN_SPLIT, val_split=VAL_SPLIT, test_split=TEST_SPLIT):\n",
    "\n",
    "    validation_split = (val_split + test_split) / (train_split + val_split + test_split)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "\n",
    "    # Gerador para dados de teste (apenas rescale)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Carregar dados de treino\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Carregar dados de validação e teste\n",
    "    temp_val_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Ajustar a proporção entre validação e teste\n",
    "    val_ratio = val_split / (val_split + test_split)\n",
    "    n_val = int(len(temp_val_generator.filenames) * val_ratio)\n",
    "    \n",
    "    # Criar geradores separados para validação e teste\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        seed=42,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDivisão do dataset:\")\n",
    "    print(f\"Treino: {len(train_generator.filenames)} imagens ({train_split*100:.1f}%)\")\n",
    "    print(f\"Validação: {n_val} imagens ({val_split*100:.1f}%)\")\n",
    "    print(f\"Teste: {len(temp_val_generator.filenames) - n_val} imagens ({test_split*100:.1f}%)\")\n",
    "\n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar o modelo com Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Carregar o modelo base MobileNetV2 pré-treinado\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "\n",
    "    # Congelar as camadas do modelo base\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Criar o modelo completo\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        # 1 neurônio para classificação binária\n",
    "        layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, validation_generator):\n",
    "    # Compilar o modelo\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina e avalia uma Random Forest para comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(train_generator, test_generator):\n",
    "    \"\"\"\n",
    "    Treina e avalia uma Random Forest para comparação\n",
    "    \"\"\"\n",
    "    # Preparar dados para Random Forest\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train_generator)):\n",
    "        batch_x, batch_y = train_generator[i]\n",
    "        X_train.extend(batch_x.reshape(batch_x.shape[0], -1))\n",
    "        y_train.extend(np.argmax(batch_y, axis=1))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Treinar Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Preparar dados de teste\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(test_generator)):\n",
    "        batch_x, batch_y = test_generator[i]\n",
    "        X_test.extend(batch_x.reshape(batch_x.shape[0], -1))\n",
    "        y_test.extend(np.argmax(batch_y, axis=1))\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Fazer predições\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    y_pred_proba_rf = rf.predict_proba(X_test)\n",
    "\n",
    "    # Calcular métricas\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "    roc_auc_rf = {}\n",
    "    for i in range(5):\n",
    "        fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba_rf[:, i])\n",
    "        roc_auc_rf[i] = auc(fpr, tpr)\n",
    "\n",
    "    return {\n",
    "        'confusion_matrix': cm_rf,\n",
    "        'roc': roc_auc_rf,\n",
    "        'report': classification_report(y_test, y_pred_rf, target_names=['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']),\n",
    "        'model': rf\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para visualização de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = 'results/'\n",
    "\n",
    "\n",
    "def plot_training_history(history, save_path=results_folder):\n",
    "    \"\"\"\n",
    "    Plota o histórico de treinamento com estilo aprimorado\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Acurácia\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], color='#2ecc71',\n",
    "             label='Treino', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], color='#e74c3c',\n",
    "             label='Validação', linewidth=2)\n",
    "    plt.title('Acurácia do Modelo', fontsize=14, pad=15)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Acurácia', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot Perda\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], color='#2ecc71',\n",
    "             label='Treino', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], color='#e74c3c',\n",
    "             label='Validação', linewidth=2)\n",
    "    plt.title('Perda do Modelo', fontsize=14, pad=15)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Perda', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path=results_folder):\n",
    "    \"\"\"\n",
    "    Plota matriz de confusão estilizada\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title('Matriz de Confusão', fontsize=14, pad=15)\n",
    "    plt.colorbar()\n",
    "\n",
    "    classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # Adicionar valores na matriz\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.xlabel('Predito', fontsize=12)\n",
    "    plt.ylabel('Real', fontsize=12)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, save_path=results_folder):\n",
    "    \"\"\"\n",
    "    Plota curva ROC estilizada\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(fpr)):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                 label=f'Classe {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='#e74c3c', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title('Curva ROC', fontsize=14, pad=15)\n",
    "    plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_models(cnn_results, rf_results, save_path='results'):\n",
    "    \"\"\"\n",
    "    Compara os resultados do CNN e Random Forest\n",
    "    Assume que:\n",
    "    - cnn_results['roc'] é uma tupla de (fpr_dict, tpr_dict, auc_dict)\n",
    "    - rf_results['roc'] é um dicionário\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Extraindo os dados ROC do CNN\n",
    "    fpr_dict, tpr_dict, roc_auc_dict = cnn_results['roc']\n",
    "\n",
    "    # Plotando curvas ROC para CNN\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    for i, (label, color) in enumerate(zip(fpr_dict.keys(), colors)):\n",
    "        fpr = fpr_dict[label]\n",
    "        tpr = tpr_dict[label]\n",
    "        roc_auc = roc_auc_dict[label]\n",
    "        plt.plot(fpr, tpr,\n",
    "                 label=f'CNN - Classe {label} (AUC = {roc_auc:.2f})',\n",
    "                 color=color, linestyle='-', linewidth=2, alpha=0.7)\n",
    "\n",
    "    # Plotando curvas ROC para Random Forest (assumindo estrutura de dicionário)\n",
    "    rf_roc = rf_results['roc']\n",
    "    for i, (label, values) in enumerate(rf_roc.items()):\n",
    "        if isinstance(values, dict):\n",
    "            fpr = values.get('fpr', [])\n",
    "            tpr = values.get('tpr', [])\n",
    "            roc_auc = values.get('auc', 0)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        plt.plot(fpr, tpr,\n",
    "                 label=f'RF - Classe {label} (AUC = {roc_auc:.2f})',\n",
    "                 color=colors[i], linestyle='--', linewidth=2, alpha=0.7)\n",
    "\n",
    "    # Configurações do gráfico\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle=':', linewidth=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
    "    plt.title('Comparação das Curvas ROC (CNN vs Random Forest)',\n",
    "              fontsize=14, pad=15)\n",
    "    plt.legend(loc='lower right', fontsize=8, bbox_to_anchor=(1.15, 0))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Salvando o gráfico\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def inspect_roc_structure(roc_data, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Função auxiliar para inspecionar a estrutura dos dados ROC\n",
    "    \"\"\"\n",
    "    print(f\"\\nInspecionando estrutura ROC - {model_name}\")\n",
    "    if isinstance(roc_data, tuple):\n",
    "        print(\"ROC é uma tupla com comprimento:\", len(roc_data))\n",
    "        for i, item in enumerate(roc_data):\n",
    "            print(f\"Item {i}:\")\n",
    "            print(\"  Tipo:\", type(item))\n",
    "            if isinstance(item, dict):\n",
    "                print(\"  Chaves:\", item.keys())\n",
    "                for k, v in item.items():\n",
    "                    print(f\"    {k}: tipo={type(v)}, \", end=\"\")\n",
    "                    if hasattr(v, 'shape'):\n",
    "                        print(f\"shape={v.shape}\")\n",
    "                    else:\n",
    "                        print(f\"len={len(v)}\")\n",
    "    elif isinstance(roc_data, dict):\n",
    "        print(\"ROC é um dicionário com chaves:\", roc_data.keys())\n",
    "        for k, v in roc_data.items():\n",
    "            print(f\"Chave {k}:\")\n",
    "            print(\"  Tipo:\", type(v))\n",
    "            if isinstance(v, dict):\n",
    "                print(\"  Subchaves:\", v.keys())\n",
    "\n",
    "\n",
    "# Função auxiliar para debugar a estrutura dos resultados\n",
    "def debug_results_structure(cnn_results, rf_results):\n",
    "    \"\"\"\n",
    "    Função para analisar a estrutura dos resultados\n",
    "    \"\"\"\n",
    "    print(\"\\nEstrutura dos resultados CNN:\")\n",
    "    print(\"Chaves disponíveis:\", cnn_results.keys())\n",
    "    print(\"\\nEstrutura do ROC CNN:\")\n",
    "    print(\"Tipo:\", type(cnn_results['roc']))\n",
    "    print(\"Comprimento:\", len(cnn_results['roc']))\n",
    "    if isinstance(cnn_results['roc'], (list, tuple)):\n",
    "        for i, item in enumerate(cnn_results['roc']):\n",
    "            print(f\"Item {i}:\", type(item), item.shape if hasattr(\n",
    "                item, 'shape') else len(item))\n",
    "\n",
    "    print(\"\\nEstrutura dos resultados RF:\")\n",
    "    print(\"Chaves disponíveis:\", rf_results.keys())\n",
    "    print(\"\\nEstrutura do ROC RF:\")\n",
    "    print(\"Tipo:\", type(rf_results['roc']))\n",
    "    print(\"Comprimento:\", len(rf_results['roc']))\n",
    "    if isinstance(rf_results['roc'], (list, tuple)):\n",
    "        for i, item in enumerate(rf_results['roc']):\n",
    "            print(f\"Item {i}:\", type(item), item.shape if hasattr(\n",
    "                item, 'shape') else len(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra informações detalhadas sobre os conjuntos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset_info(train_generator, val_generator, test_generator):\n",
    "    \"\"\"\n",
    "    Mostra informações detalhadas sobre os conjuntos de dados\n",
    "    \"\"\"\n",
    "    total_images = (len(train_generator.filenames) +\n",
    "                    len(val_generator.filenames) +\n",
    "                    len(test_generator.filenames))\n",
    "\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INFORMAÇÕES DO DATASET\".center(50))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(f\"\\nTotal de imagens: {total_images}\")\n",
    "    print(f\"Quantidade de classes: {num_classes}\")\n",
    "\n",
    "    # Informações do conjunto de treino\n",
    "    print(\"\\n\" + \"-\"*20 + \" TREINO \" + \"-\"*20)\n",
    "    print(f\"Total: {len(train_generator.filenames)} imagens\")\n",
    "    print(f\"Proporção: {len(train_generator.filenames)/total_images:.1%}\")\n",
    "    class_dist = dict(zip(train_generator.class_indices.keys(),\n",
    "                          np.bincount(train_generator.classes)))\n",
    "    for cls, count in class_dist.items():\n",
    "        print(f\"{cls}: {count} imagens ({\n",
    "              count/len(train_generator.filenames):.1%})\")\n",
    "\n",
    "    # Informações do conjunto de validação\n",
    "    print(\"\\n\" + \"-\"*20 + \" VALIDAÇÃO \" + \"-\"*20)\n",
    "    print(f\"Total: {len(val_generator.filenames)} imagens\")\n",
    "    print(f\"Proporção: {len(val_generator.filenames)/total_images:.1%}\")\n",
    "    class_dist = dict(zip(val_generator.class_indices.keys(),\n",
    "                          np.bincount(val_generator.classes)))\n",
    "    for cls, count in class_dist.items():\n",
    "        print(f\"{cls}: {count} imagens ({count/len(val_generator.filenames):.1%})\")\n",
    "\n",
    "    # Informações do conjunto de teste\n",
    "    print(\"\\n\" + \"-\"*20 + \" TESTE \" + \"-\"*20)\n",
    "    print(f\"Total: {len(test_generator.filenames)} imagens\")\n",
    "    print(f\"Proporção: {len(test_generator.filenames)/total_images:.1%}\")\n",
    "    class_dist = dict(zip(test_generator.class_indices.keys(),\n",
    "                          np.bincount(test_generator.classes)))\n",
    "    for cls, count in class_dist.items():\n",
    "        print(f\"{cls}: {count} imagens ({count/len(test_generator.filenames):.1%})\")\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_generator):\n",
    "    \"\"\"\n",
    "    Avalia o modelo usando várias métricas\n",
    "    \"\"\"\n",
    "    # Fazer predições\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    # Calcular matriz de confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calcular ROC e AUC para cada classe\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    for i in range(len(test_generator.class_indices)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true == i, predictions[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Gerar relatório de classificação\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=test_generator.class_indices.keys())\n",
    "\n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'roc': (fpr, tpr, roc_auc),\n",
    "        'report': report,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula as métricas de avaliação do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula as métricas de avaliação do modelo\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    n_classes = cm.shape[0]\n",
    "    metrics = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1_score = 2 * (precision * sensitivity) / (precision +\n",
    "                                                    sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "        metrics[i] = {\n",
    "            'Acurácia': accuracy,\n",
    "            'Sensibilidade': sensitivity,\n",
    "            'Especificidade': specificity,\n",
    "            'Precisão': precision,\n",
    "            'F-score': f1_score\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_detailed_metrics(metrics, class_names, save_path='results'):\n",
    "    \"\"\"\n",
    "    Gera visualizações detalhadas das métricas\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Preparando dados para visualização\n",
    "    metric_names = ['Acurácia', 'Sensibilidade',\n",
    "                    'Especificidade', 'Precisão', 'F-score']\n",
    "    metrics_data = np.zeros((len(class_names), len(metric_names)))\n",
    "\n",
    "    for i in range(len(class_names)):\n",
    "        for j, metric in enumerate(metric_names):\n",
    "            metrics_data[i, j] = metrics[i][metric]\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(metrics_data, annot=True, fmt='.3f', xticklabels=metric_names,\n",
    "                yticklabels=class_names, cmap='YlOrRd')\n",
    "    plt.title('Métricas de Avaliação por Classe')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/detailed_metrics_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Gráfico de barras\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.15\n",
    "\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        values = [metrics[j][metric] for j in range(len(class_names))]\n",
    "        plt.bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Valores')\n",
    "    plt.title('Métricas de Avaliação por Classe')\n",
    "    plt.xticks(x + width*2, class_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/detailed_metrics_bars.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def display_detailed_metrics(metrics, class_names):\n",
    "    \"\"\"\n",
    "    Exibe um resumo textual detalhado das métricas\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MÉTRICAS DETALHADAS DE AVALIAÇÃO\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    avg_metrics = {metric: 0 for metric in metrics[0].keys()}\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"\\nClasse: {class_name}\")\n",
    "        print(\"-\"*30)\n",
    "\n",
    "        for metric, value in metrics[i].items():\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "            avg_metrics[metric] += value\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MÉDIAS GERAIS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    n_classes = len(class_names)\n",
    "    for metric, total in avg_metrics.items():\n",
    "        avg = total / n_classes\n",
    "        print(f\"Média {metric}: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para fazer previsões com novas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    # Carregar o modelo salvo\n",
    "    model = tf.keras.models.load_model('models/flowers_classifier.keras')\n",
    "\n",
    "    # Carregar e preprocessar a imagem\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0  # Normalização\n",
    "\n",
    "    # Fazer a predição\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Interpretar o resultado\n",
    "    # Como usamos classificação com 5 classes, usamos argmax para obter a classe com maior probabilidade\n",
    "    class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "    predicted_class_index = np.argmax(prediction[0])\n",
    "    predicted_class = class_names[predicted_class_index]\n",
    "    confidence = prediction[0][predicted_class_index]\n",
    "\n",
    "    return predicted_class, float(confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função principal para treinar e avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_split=TRAIN_SPLIT, val_split=VAL_SPLIT, test_split=TEST_SPLIT):\n",
    "\n",
    "    # Definir o diretório dos dados\n",
    "    data_dir = 'dataset/Flowers/train'\n",
    "\n",
    "    # Preparar os dados com os splits especificados\n",
    "    train_generator, validation_generator, test_generator = prepare_data(\n",
    "        data_dir,\n",
    "        train_split=train_split,\n",
    "        val_split=val_split,\n",
    "        test_split=test_split\n",
    "    )\n",
    "\n",
    "    # Mostrar informações do dataset\n",
    "    show_dataset_info(train_generator, validation_generator, test_generator)\n",
    "\n",
    "    # Criar o modelo\n",
    "    model = create_model()\n",
    "    history = train_model(model, train_generator, validation_generator)\n",
    "\n",
    "    # Avaliar o modelo CNN\n",
    "    cnn_results = evaluate_model(model, test_generator)\n",
    "    \n",
    "    # Obter predições para métricas detalhadas\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    # Calcular e mostrar métricas detalhadas\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    detailed_metrics = calculate_metrics(y_true, y_pred)\n",
    "    \n",
    "    # Plotar métricas detalhadas\n",
    "    plot_detailed_metrics(detailed_metrics, class_names)\n",
    "    \n",
    "    # Mostrar resumo das métricas\n",
    "    display_detailed_metrics(detailed_metrics, class_names)\n",
    "    \n",
    "    # Treinar e avaliar Random Forest\n",
    "    rf_results = train_random_forest(train_generator, test_generator)\n",
    "\n",
    "    # Plotar resultados originais\n",
    "    plot_training_history(history, save_path='results/training_history.png')\n",
    "    plot_confusion_matrix(\n",
    "        cnn_results['confusion_matrix'], save_path='results/cnn_confusion_matrix.png')\n",
    "    plot_roc_curve(cnn_results['roc'][0], cnn_results['roc'][1],\n",
    "                   cnn_results['roc'][2], save_path='results/cnn_roc_curve.png')\n",
    "    \n",
    "    inspect_roc_structure(cnn_results['roc'], \"CNN\")\n",
    "    inspect_roc_structure(rf_results['roc'], \"Random Forest\")\n",
    "    \n",
    "    compare_models(cnn_results, rf_results, save_path='results/model_comparison.png')\n",
    "\n",
    "    # Imprimir relatórios de classificação\n",
    "    print(\"\\nRelatório de Classificação - CNN:\")\n",
    "    print(cnn_results['report'])\n",
    "    print(\"\\nRelatório de Classificação - Random Forest:\")\n",
    "    print(rf_results['report'])\n",
    "\n",
    "    # Avaliar no conjunto de teste\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "    print(f'\\nDesempenho no conjunto de teste:')\n",
    "    print(f'Acurácia: {test_accuracy:.4f}')\n",
    "    print(f'Loss: {test_loss:.4f}')\n",
    "\n",
    "    # Salvar o modelo\n",
    "    model.save('models/flowers_classifier.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução da função principal para treinar e avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo com os splits padrão\n",
    "main(train_split=0.7, val_split=0.15, test_split=0.15)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMX++NYdbdz0nMRwSSbW7N8",
   "mount_file_id": "1yVMYPoreO8806AR5cEUL_I5zuFNTr6P4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
